import pandas as pd
import numpy as np
from underthesea import word_tokenize, pos_tag, sent_tokenize
import regex
import demoji
from pyvi import ViPosTagger, ViTokenizer
import string
import re

def load_lst():
    # Load Emojicon
    file = open('data/files/emojicon.txt', 'r', encoding='utf-8')
    emoji_lst = file.read().split('\n')
    emoji_dict = {}
    for line in emoji_lst:
        key, value = line.split('\t')
        emoji_dict[key] = value
    file.close()

    # Load Teencode
    file = open('data/files/teencode.txt', 'r', encoding='utf-8')
    teencode_lst = file.read().split('\n')
    teencode_dict = {}
    for line in teencode_lst:
        key, value = line.split('\t')
        teencode_dict[key] = value
    file.close()

    # Load Translate English to Vietnamese
    file = open('data/files/english-vnmese.txt', 'r', encoding='utf-8')
    eng_vnmese_lst = file.read().split('\n')
    eng_vnmese_dict = {}
    for line in eng_vnmese_lst:
        key, value = line.split('\t')
        eng_vnmese_dict[key] = value
    file.close()

    # Load Wrong Words
    file = open('data/files/wrong-word.txt', 'r', encoding='utf-8')
    wrong_word_lst = file.read().split('\n')
    file.close()

    # Load stop words
    file = open('data/files/vietnamese-stopwords.txt', 'r', encoding='utf-8')
    stop_word_lst = file.read().split('\n')
    file.close()

    return emoji_dict, teencode_dict, eng_vnmese_dict, wrong_word_lst, stop_word_lst



def process_text(text, emoji_dict, teen_dict, wrong_lst, eng_vnmese_dict):    
    document = text.lower()    
    document = document.replace("â€™",'')    
    document = regex.sub(r'\.+', ".", document)    
    new_sentence =''    
    for sentence in sent_tokenize(document):        
        # if not(sentence.isascii()):
        # Add space before emoji
        sentence = regex.sub(r'(?<=[^\W\d_])\b', ' ', sentence)

        ###### CONVERT EMOJICON        
        sentence = ''.join(emoji_dict[word]+' ' if word in emoji_dict else word for word in list(sentence)) 

        ###### Remove emoji
        sentence = demoji.replace(sentence, '') 

        ###### CONVERT ENGLISH TO VIETNAMESE
        sentence = ' '.join(eng_vnmese_dict[word] if word in eng_vnmese_dict else word for word in sentence.split())

        ###### CONVERT TEENCODE        
        sentence = ' '.join(teen_dict[word] if word in teen_dict else word for word in sentence.split())        
        
        ###### DEL Punctuation & Numbers        
        pattern = r'(?i)\b[a-zÃ¡Ã áº£Ã£áº¡Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ã©Ã¨áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡Ã³Ã²á»Ãµá»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£Ã­Ã¬á»‰Ä©á»‹ÃºÃ¹á»§Å©á»¥Æ°á»©á»«á»­á»¯á»±Ã½á»³á»·á»¹á»µÄ‘]+\b'        
        sentence = ' '.join(regex.findall(pattern,sentence))        
        # ...        
        ###### DEL wrong words        
        # sentence = ' '.join('' if word in wrong_lst else word for word in sentence.split())    
            
        new_sentence = new_sentence+ sentence + '. '    
    document = new_sentence    
    #print(document)    
    ###### DEL excess blank space    
    document = regex.sub(r'\s+', ' ', document).strip()    
    #...    
    return document

# Chuáº©n hÃ³a unicode tiáº¿ng viá»‡t
def loaddicchar():
    uniChars = "Ã Ã¡áº£Ã£áº¡Ã¢áº§áº¥áº©áº«áº­Äƒáº±áº¯áº³áºµáº·Ã¨Ã©áº»áº½áº¹Ãªá»áº¿á»ƒá»…á»‡Ä‘Ã¬Ã­á»‰Ä©á»‹Ã²Ã³á»Ãµá»Ã´á»“á»‘á»•á»—á»™Æ¡á»á»›á»Ÿá»¡á»£Ã¹Ãºá»§Å©á»¥Æ°á»«á»©á»­á»¯á»±á»³Ã½á»·á»¹á»µÃ€Ãáº¢Ãƒáº Ã‚áº¦áº¤áº¨áºªáº¬Ä‚áº°áº®áº²áº´áº¶ÃˆÃ‰áººáº¼áº¸ÃŠá»€áº¾á»‚á»„á»†ÄÃŒÃá»ˆÄ¨á»ŠÃ’Ã“á»ŽÃ•á»ŒÃ”á»’á»á»”á»–á»˜Æ á»œá»šá»žá» á»¢Ã™Ãšá»¦Å¨á»¤Æ¯á»ªá»¨á»¬á»®á»°á»²Ãá»¶á»¸á»´Ã‚Ä‚ÄÃ”Æ Æ¯"
    unsignChars = "aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU"

    dic = {}
    char1252 = 'aÌ€|aÌ|aÌ‰|aÌƒ|aÌ£|Ã¢Ì€|Ã¢Ì|Ã¢Ì‰|Ã¢Ìƒ|Ã¢Ì£|ÄƒÌ€|ÄƒÌ|ÄƒÌ‰|ÄƒÌƒ|ÄƒÌ£|eÌ€|eÌ|eÌ‰|eÌƒ|eÌ£|ÃªÌ€|ÃªÌ|ÃªÌ‰|ÃªÌƒ|ÃªÌ£|iÌ€|iÌ|iÌ‰|iÌƒ|iÌ£|oÌ€|oÌ|oÌ‰|oÌƒ|oÌ£|Ã´Ì€|Ã´Ì|Ã´Ì‰|Ã´Ìƒ|Ã´Ì£|Æ¡Ì€|Æ¡Ì|Æ¡Ì‰|Æ¡Ìƒ|Æ¡Ì£|uÌ€|uÌ|uÌ‰|uÌƒ|uÌ£|Æ°Ì€|Æ°Ì|Æ°Ì‰|Æ°Ìƒ|Æ°Ì£|yÌ€|yÌ|yÌ‰|yÌƒ|yÌ£|AÌ€|AÌ|AÌ‰|AÌƒ|AÌ£|Ã‚Ì€|Ã‚Ì|Ã‚Ì‰|Ã‚Ìƒ|Ã‚Ì£|Ä‚Ì€|Ä‚Ì|Ä‚Ì‰|Ä‚Ìƒ|Ä‚Ì£|EÌ€|EÌ|EÌ‰|EÌƒ|EÌ£|ÃŠÌ€|ÃŠÌ|ÃŠÌ‰|ÃŠÌƒ|ÃŠÌ£|IÌ€|IÌ|IÌ‰|IÌƒ|IÌ£|OÌ€|OÌ|OÌ‰|OÌƒ|OÌ£|Ã”Ì€|Ã”Ì|Ã”Ì‰|Ã”Ìƒ|Ã”Ì£|Æ Ì€|Æ Ì|Æ Ì‰|Æ Ìƒ|Æ Ì£|UÌ€|UÌ|UÌ‰|UÌƒ|UÌ£|Æ¯Ì€|Æ¯Ì|Æ¯Ì‰|Æ¯Ìƒ|Æ¯Ì£|YÌ€|YÌ|YÌ‰|YÌƒ|YÌ£'.split(
        '|')
    charutf8 = "Ã |Ã¡|áº£|Ã£|áº¡|áº§|áº¥|áº©|áº«|áº­|áº±|áº¯|áº³|áºµ|áº·|Ã¨|Ã©|áº»|áº½|áº¹|á»|áº¿|á»ƒ|á»…|á»‡|Ã¬|Ã­|á»‰|Ä©|á»‹|Ã²|Ã³|á»|Ãµ|á»|á»“|á»‘|á»•|á»—|á»™|á»|á»›|á»Ÿ|á»¡|á»£|Ã¹|Ãº|á»§|Å©|á»¥|á»«|á»©|á»­|á»¯|á»±|á»³|Ã½|á»·|á»¹|á»µ|Ã€|Ã|áº¢|Ãƒ|áº |áº¦|áº¤|áº¨|áºª|áº¬|áº°|áº®|áº²|áº´|áº¶|Ãˆ|Ã‰|áºº|áº¼|áº¸|á»€|áº¾|á»‚|á»„|á»†|ÃŒ|Ã|á»ˆ|Ä¨|á»Š|Ã’|Ã“|á»Ž|Ã•|á»Œ|á»’|á»|á»”|á»–|á»˜|á»œ|á»š|á»ž|á» |á»¢|Ã™|Ãš|á»¦|Å¨|á»¤|á»ª|á»¨|á»¬|á»®|á»°|á»²|Ã|á»¶|á»¸|á»´".split(
        '|')
    for i in range(len(char1252)):
        dic[char1252[i]] = charutf8[i]
    return dic

# ÄÆ°a toÃ n bá»™ dá»¯ liá»‡u qua hÃ m nÃ y Ä‘á»ƒ chuáº©n hÃ³a láº¡i
def covert_unicode(txt):
    dicchar = loaddicchar()
    return regex.sub(
        r'aÌ€|aÌ|aÌ‰|aÌƒ|aÌ£|Ã¢Ì€|Ã¢Ì|Ã¢Ì‰|Ã¢Ìƒ|Ã¢Ì£|ÄƒÌ€|ÄƒÌ|ÄƒÌ‰|ÄƒÌƒ|ÄƒÌ£|eÌ€|eÌ|eÌ‰|eÌƒ|eÌ£|ÃªÌ€|ÃªÌ|ÃªÌ‰|ÃªÌƒ|ÃªÌ£|iÌ€|iÌ|iÌ‰|iÌƒ|iÌ£|oÌ€|oÌ|oÌ‰|oÌƒ|oÌ£|Ã´Ì€|Ã´Ì|Ã´Ì‰|Ã´Ìƒ|Ã´Ì£|Æ¡Ì€|Æ¡Ì|Æ¡Ì‰|Æ¡Ìƒ|Æ¡Ì£|uÌ€|uÌ|uÌ‰|uÌƒ|uÌ£|Æ°Ì€|Æ°Ì|Æ°Ì‰|Æ°Ìƒ|Æ°Ì£|yÌ€|yÌ|yÌ‰|yÌƒ|yÌ£|AÌ€|AÌ|AÌ‰|AÌƒ|AÌ£|Ã‚Ì€|Ã‚Ì|Ã‚Ì‰|Ã‚Ìƒ|Ã‚Ì£|Ä‚Ì€|Ä‚Ì|Ä‚Ì‰|Ä‚Ìƒ|Ä‚Ì£|EÌ€|EÌ|EÌ‰|EÌƒ|EÌ£|ÃŠÌ€|ÃŠÌ|ÃŠÌ‰|ÃŠÌƒ|ÃŠÌ£|IÌ€|IÌ|IÌ‰|IÌƒ|IÌ£|OÌ€|OÌ|OÌ‰|OÌƒ|OÌ£|Ã”Ì€|Ã”Ì|Ã”Ì‰|Ã”Ìƒ|Ã”Ì£|Æ Ì€|Æ Ì|Æ Ì‰|Æ Ìƒ|Æ Ì£|UÌ€|UÌ|UÌ‰|UÌƒ|UÌ£|Æ¯Ì€|Æ¯Ì|Æ¯Ì‰|Æ¯Ìƒ|Æ¯Ì£|YÌ€|YÌ|YÌ‰|YÌƒ|YÌ£',
        lambda x: dicchar[x.group()], txt)


def process_special_word(text):
    # cÃ³ thá»ƒ cÃ³ nhiá»u tá»« Ä‘áº·c biá»‡t cáº§n rÃ¡p láº¡i vá»›i nhau
    new_text = ''
    text_lst = text.split()
    i= 0
    # khÃ´ng, cháº³ng, cháº£...
    if 'khÃ´ng' in text_lst:
        while i <= len(text_lst) - 1:
            word = text_lst[i]
            #print(word)
            #print(i)
            if  word == 'khÃ´ng':
                next_idx = i+1
                if next_idx <= len(text_lst) -1:
                    word = word +'_'+ text_lst[next_idx]
                i= next_idx + 1
            else:
                i = i+1
            new_text = new_text + word + ' '
    else:
        new_text = text
    return new_text.strip()


def normalize_repeated_characters(text):
    # xá»­ lÃ½ cÃ¡c tá»« cÃ³ kÃ½ tá»± láº·p láº¡i nhiá»u láº§n
    return re.sub(r'(.+?)\1+', r'\1', text)

# https://viettelgroup.ai/document/part-of-speech-tagging
def process_postag_thesea(text):
    new_document = ''
    for sentence in sent_tokenize(text):
        sentence = sentence.replace('.', '')
        ## POS tag
        lst_word_type = ['N', 'NP', 'A', 'AB' , 'AY' , 'V', 'VB', 'VY', 'R', 'M' ]
        # lst_word_type = [ 'N', 'NP', 'A', 'AB' , 'AY' , 'ABY', 'V', 'VB', 'VY', 'R' , 'M', 'I']
        # print(pos_tag(process_special_word(word_tokenize(sentence, format="text"))))
        sentence = ' '.join(word[0] if word[1].upper() in lst_word_type else '' for word in pos_tag(process_special_word(word_tokenize(sentence, format="text"))))
        new_document = new_document + sentence + ' '
    # Delete excess blank space
    new_document = regex.sub(r'\s+', ' ', new_document).strip()
    return new_document

def process_postag_thesea_adj(text):
    new_document = ''
    for sentence in sent_tokenize(text):
        sentence = sentence.replace('.', '')
        ## POS tag
        lst_word_type = ['A', 'AB' , 'AY' , 'ABY', 'M' , 'R']
        # lst_word_type = [ 'N', 'NP', 'A', 'AB' , 'AY' , 'ABY', 'V', 'VB', 'VY', 'R' , 'M', 'I']
        # print(pos_tag(process_special_word(word_tokenize(sentence, format="text"))))
        sentence = ' '.join(word[0] if word[1].upper() in lst_word_type else '' for word in pos_tag(process_special_word(word_tokenize(sentence, format="text"))))
        new_document = new_document + sentence + ' '
    # Delete excess blank space
    new_document = regex.sub(r'\s+', ' ', new_document).strip()
    return new_document

def remove_stopwords(text, stop_word_lst):
    doc= ' '.join('' if word in stop_word_lst else word for word in text.split())
    doc = regex.sub(r'\s+', ' ', doc).strip()
    return doc

def preprocess_text_all_together(text):
    emoji_dict, teen_dict, eng_vnmese_dict, wrong_lst, stop_word_lst = load_lst()
    # print("# Original text: ")
    # print(text)
    text = process_text(text, emoji_dict, teen_dict, wrong_lst, eng_vnmese_dict)
    # print("# After process_text: ")
    # print(text)
    # text = convert_unicode(text)
    # print("# After convert_unicode: ")
    # print(text)
    text = process_special_word(text)
    # print("# After process_special_word: ")
    # print(text)

    text = normalize_repeated_characters(text)
    # print("# After normalize_repeated_characters: ")
    # print(text)

    text = process_postag_thesea(text)
    # print("# After process_postag_thesea: ")
    # print(text)
    text = remove_stopwords(text, stop_word_lst)
    # print("# After remove_stopwords: ")
    # print(text)
    return text


text = "ÄÃ£ thÆ° ráº¥t ngon"
text2 = '''21h30...2 Ä‘á»©a nhá» kÃªu Ä‘Ã³i, sau 1 há»“i bÃ¬nh loáº¡n lÃ  chá»‘t McDonal!
Äang hÃ¡o há»©c vÃ¬ 2 Ä‘á»©a nÃ³ vá»«a Äƒn vá»«a khen láº¥y khen Ä‘á»ƒ, váº­y mÃ  ná»¡ lÃ²ng nÃ o Ä‘áº¿n lÆ°á»£t mÃ¬nh nÃ³ láº¡i thÃ nh ra tháº¿ nÃ yyyyðŸ˜­ðŸ˜­ðŸ˜­
CÆ¡m sá»‘ng toÃ n táº­p, Äƒn cÆ¡m mÃ  cá»© nhÆ° nhai gáº¡o... ChÃ¡n chá»‹u ko ná»•iðŸ˜¤ðŸ˜¤ðŸ˜¤'''
text3 = "ÄÃ£ Äƒn qua rá»“i, Äƒn ok áº¡	"
preprocess_text_all_together("GÃ  chiÃªn cÃ²n sá»‘ng, ráº¥t tanh. Khá»§ng khiÃ©p	")
